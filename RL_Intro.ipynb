{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction to Reinforcement Learning with TensorFlow</h1>\n",
    "<p>This tutorial was adapted from various <a href=\"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724\">examples</a> provided by Arthur Juliani. I would recommend their various blog posts for further reading on Reinforcement Learning.</p>\n",
    "<p>The following notebook introduces some of the key concepts of Reinforcment Learning with Python and TensorFlow. Please make sure that the following import statements run successfully before continuing:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have this taken care of, we can begin  with a brief introduction to one subset of Reinforcement Learning methods: Policy based methods. Policy based methods allow us to make our agent improve its performance by determining which policies for acting produce better results. This is different from more state based methods referred to as Q learning methods. <a href=\"https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html\">This writeup</a> provides a relatively concise comparision of the two methods, but in short: policy methods seek to act optimally while Q learning methods seek to reach optimal states. Both methods may be used to make an intelligent agent, but for the examples we are working with here, our state space is more continuous than discrete.</p>\n",
    "<p>Our agent should operate by processing its current <i>observation</i> of the world to choose <i>actions</i> which maximize a <i>reward</i> feedback symbol. The specific world our agent will work in is referred to as the <i>environment</i>, and for this example, we will be working with OpenAI's <a href=\"https://gym.openai.com/docs/\">Cart-Pole</a> environment. Running the following code should set up our example environment.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have our environment set up, we can take a look at what it looks like using the render() function. For this environment, our goal is to train an agent to balance a pivoting pole on a rolling cart. The following code will reset our environment and use an agent which randomly performs actions for 20 episodes, each composed of 100 frames.</p>\n",
    "\n",
    "<p>Note: if the following code fails, try downgrading the pyglet package using the following command:</p>\n",
    "<div style=\"background-color:#300a24\"><b><p style=\"color:white\">python3 -m pip install pyglet==1.2.4</p></b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 12 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 17 timesteps\n",
      "Episode finished after 25 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 19 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 25 timesteps\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Observation Space</h1>\n",
    "<p>It is important to note what our observation space is. Our observations represent how our agent perceives the world, and in the case of the CartPole environment, our observations consist of 4 numbers representing the position of the cart, the velocity of the cart, the angle of the pole, and the rotational velocity of the pole. Additional information about the CartPole environment can be found <a href=\"https://github.com/openai/gym/wiki/CartPole-v0\">here</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print('Observation space:', env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
