{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction to Reinforcement Learning with TensorFlow</h1>\n",
    "<p>This tutorial was adapted from various <a href=\"https://medium.com/@awjuliani/super-simple-reinforcement-learning-tutorial-part-2-ded33892c724\">examples</a> provided by Arthur Juliani. I would recommend their various blog posts for further reading on Reinforcement Learning.</p>\n",
    "<p>The following notebook introduces some of the key concepts of Reinforcment Learning with Python and TensorFlow. Please make sure that the following import statements run successfully before continuing:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have this taken care of, we can begin  with a brief introduction to one subset of Reinforcement Learning methods: Policy based methods. Policy based methods allow us to make our agent improve its performance by determining which policies for acting produce better results. This is different from more state based methods referred to as Q learning methods. <a href=\"https://flyyufelix.github.io/2017/10/12/dqn-vs-pg.html\">This writeup</a> provides a relatively concise comparision of the two methods, but in short: policy methods seek to act optimally while Q learning methods seek to reach optimal states. Both methods may be used to make an intelligent agent, but for the examples we are working with here, our state space is more continuous than discrete.</p>\n",
    "<p>Our agent should operate by processing its current <i>observation</i> of the world to choose <i>actions</i> which maximize a <i>reward</i> feedback symbol. The specific world our agent will work in is referred to as the <i>environment</i>, and for this example, we will be working with OpenAI's <a href=\"https://gym.openai.com/docs/\">Cart-Pole</a> environment. Running the following code should set up our example environment.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Now that we have our environment set up, we can take a look at what it looks like using the render() function. For this environment, our goal is to train an agent to balance a pivoting pole on a rolling cart. The following code will reset our environment and use an agent which randomly performs actions for 20 episodes, each composed of 100 frames.</p>\n",
    "\n",
    "<p>Note: if the following code fails, try downgrading the pyglet package using the following command:</p>\n",
    "<div style=\"background-color:#300a24\"><b><p style=\"color:white\">python3 -m pip install pyglet==1.2.4</p></b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 13 timesteps\n",
      "Episode finished after 14 timesteps\n",
      "Episode finished after 21 timesteps\n",
      "Episode finished after 20 timesteps\n",
      "Episode finished after 39 timesteps\n",
      "Episode finished after 28 timesteps\n",
      "Episode finished after 41 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 12 timesteps\n",
      "Episode finished after 32 timesteps\n",
      "Episode finished after 11 timesteps\n",
      "Episode finished after 38 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 10 timesteps\n",
      "Episode finished after 13 timesteps\n",
      "Episode finished after 18 timesteps\n",
      "Episode finished after 15 timesteps\n",
      "Episode finished after 16 timesteps\n",
      "Episode finished after 12 timesteps\n"
     ]
    }
   ],
   "source": [
    "for i_episode in range(20):\n",
    "    observation = env.reset()\n",
    "    for t in range(100):\n",
    "        env.render()\n",
    "        action = env.action_space.sample()\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Observation Space</h1>\n",
    "<p>It is important to note what our observation space is. Our observations represent how our agent perceives the world, and in the case of the CartPole environment, our observations consist of 4 numbers representing the position of the cart, the velocity of the cart, the angle of the pole, and the rotational velocity of the pole. Additional information about the CartPole environment can be found <a href=\"https://github.com/openai/gym/wiki/CartPole-v0\">here</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print('Observation space:', env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Action Space</h1>\n",
    "<p>The other key aspect our AI needs to understand is the action space, which represents what choices our agent has each timestep. In this example, the cart has only 2 options: push the cart left or push the cart right.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space: Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print('Action space:', env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Rewards</h1>\n",
    "<p>Now that we know what the world looks like and what our options are, we need to understand how we rate our agent's performance. In this example, the agent receives a reward of 1 point for each timestep. The episode will continue to give the agent rewards until either of the following occur:</p>\n",
    "<ol>\n",
    "<li>The cart moves too far left or right off screen.</li>\n",
    "<li>The pole falls over.</li>\n",
    "<li>The episode reaches 200 timesteps.</li>\n",
    "</ol>\n",
    "<p>As a result, we should expect that an intelligent agent will work to stay on screen while keeping the pole balanced upright for as long as it can (up to 200 timesteps). In order to optimally train our agent, we train the agent using to maximize its expected rewards by backpropogating discounted rewards. This means that after each episode, we look back at our experience and add our final reward discounted by our parameter gamma ($\\gamma$).</p>\n",
    "<p>Gamma can be interpretted as how far/near -sighted our agent is. With a value close to 0, our agent does not care as much about future rewards. With a value close to 1, our agent cares more about future rewards. Typically, you would want a value close to, but not necessarily 1. For this example, we choose $\\gamma=0.99$.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gamma = 0.99\n",
    "\n",
    "def discount_rewards(r):\n",
    "    \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n",
    "    discounted_r = np.zeros_like(r)\n",
    "    running_add = 0\n",
    "    for t in reversed(xrange(0, r.size)):\n",
    "        running_add = running_add * gamma + r[t]\n",
    "        discounted_r[t] = running_add\n",
    "    return discounted_r"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
